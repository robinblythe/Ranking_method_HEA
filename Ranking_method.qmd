---
title: "Ranking method simulation study"
author: "Robin Blythe"
format: html
editor: visual
bibliography: references.bib
---

## Background

This project is an exploratory study to determine whether ranking patients by predicted risk can yield lower costs and better outcomes than using thresholds from a hospital perspective. Thresholds, while convenient to implement, may be less cost-effective than ranking, given that a ranking system should theoretically be capable of identifying true positives first from a discriminating model. Prediction models are capable of generating rich outcome data, but dichotomising into high and low risk does not fully utilise predictions.

A recent consideration with regards to implementing thresholds for risk stratification has been to limit the number of alerts based on what clinicians will tolerate, using the positive predictive value (PPV) to determine number needed to evaluate (NNE).[@romero-brufau2015] For example, a model with a PPV of 0.1 corresponds to a rate of 1 positive per 10 alerts, or a NNE of 10. This approach can be considered in addition to other, ROC or value-based methods. [@Parsons2023]

This study contrasts the two approaches: what is the economic impact of applying a ranking approach compared to a threshold approach for a given sample population?

## Study outline

1.  Determine a use case.
    i.  In this study, we will examine the implementation of a program to identify patients at high risk of deterioration leading to potentially avoidable additional costs and poor outcomes. We can use the implementation of Escobar et al (2020) to obtain prevalence for ICU admission, a common endpoint for deterioration prediction. [@Escobar2020]
    ii. Clinical deterioration is characterised by several challenges including which outcome to use and, depending on the selected outcome, a low prevalence of the event. [@Blythe2023] However, this has not dissuaded hospitals from applying clinical deterioration models, despite the lack of success in improving patient outcomes. [@Blythe2022]
2.  Simulate a hypothetical clinical prediction model with a given AUC and underlying prevalence.
3.  Simulate an underlying patient population using the minimum required sample size.
4.  Obtain costs and outcomes for each state: true positive, true negative, false positive, false negative.
5.  Identify a selection of thresholds based on the study population used to generate the model.
    i.  Value-optimising threshold
    ii. Youden index (or other ROC-based method)
    iii. PPV/NNE-based approach
6.  Randomly generate new sample populations based on a range of plausible AUC and prevalence values to mimic external validation.
7.  Apply the threshold obtained from step 5 and the ranking approach, obtaining costs and outcomes for each sample.
    i.  To apply the ranking approach, we can derive a function/visualisation for the incremental NMB for each patient evaluated. Clinicians can move down the list until they reach a discretionary point where additional evaluation is not warranted.
    ii. To apply the threshold approach, each patient MUST be evaluated. This is the difference with the ranking approach, which is discretionary. Clinicians can rank patients above the threshold, but not below it.
8.  Compare NMB across variety of AUCs and prevalence for both ranking and threshold-based methods.

## Methods

Let's assume that, left unassessed, a deteriorating medical inpatient will need to be rapidly transferred to the ICU, which incurs significant additional costs. A deterioration detection system can escalate that decision to an outreach team, in this case comprising of an ICU registrar, the patient's attending, and the patient's nurse.

False positives can lead to wasted clinician time, and false negatives can lead to costly ICU admissions. Considering that many patients will require ICU admission regardless of whether they can be successfully intervened upon, we should estimate a conservative treatment effect for assessment, rather than assuming an early intervention will necessarily be preventative.

### Set up experiment

```{r, message = FALSE}
options(scipen = 999, digits = 3)
library(predictNMB)
library(pROC)
library(tidyverse)

# Hypothetical model specs and event rate
auc = 0.85
params = 30
p0 = 0.05
wtp = ceiling(28033*(1.03)^(2024 - 2018))

# Obtain minimum sample size/events
pmsamp <- pmsampsize::pmsampsize(
  type = "b",
  prevalence = p0, 
  cstatistic = auc,
  parameters = params)

sample_size <- pmsamp$sample_size
min_events <- ceiling(pmsamp$events)
```

For a model with `r params` parameters, an AUC of `r auc` and a prevalence of `r p0`, we need a sample size of `r format(sample_size, big.mark = ",")` with `r min_events` events. We can use these requirements to generate a hypothetical study population.

### Generate simulated dataset

```{r}
set.seed(888)

# Use model specs and minimum sample size to generate a sample population
sample_pop <- get_sample(
  auc = auc, 
  n_samples = sample_size, 
  prevalence = p0, 
  min_events = min_events)

sample_pop$actual <- factor(sample_pop$actual)

# Obtain predicted probabilities
fit <- glm(actual ~ x, 
           data = sample_pop, 
           family = binomial()
           )
sample_pop$predicted <- predict(fit, type = "response")
```

### Assign costs and outcomes to 2 x 2 table

#### Parameters

-   Additional costs due to deterioration
    -   The additional utilisation cost of deterioration is \~ð’©(14134, 686) [@Curtis2021]
    -   This cost is in addition to a clinical ICU assessment (see below)
-   Cost of clinical time per minute based on ICU outreach registrar, the resident medical officer, and the patient's nurse [@Bohingamu2024]
    -   ICU outreach registrar hourly: 69.91/60 \* (1.03)\^(2024 - 2016) = 1.48
    -   Resident medical officer hourly: 50.79/60 \* (1.03)\^(2024 - 2016) = 1.07
    -   Nurse hourly: 30.47 \* (1.03)\^(2024 - 2016) = 0.64
    -   Altogether: \$3.19/minute (fixed)
-   Duration of clinical assessment (using MET call time) \~ðšª(110.314, 0.172) [@Bellomo2003]
    -   Note that this was converted to a Gamma distribution based on a mean duration of 19 minutes and a standard error of 18/âˆš99 = 1.809 minutes using ShinyPrior [@white2023]
-   Effect of alert on ICU admission rate (hazard ratio) \~ð’©(0.910, 0.036) [@Escobar2020]
    -   Note: must be taken as (1 - HR) for sampler
-   QALYs lost from deterioration episode \~ð’©(0.03, 0.04) [@Holmes2024]
-   Opportunity cost of a positive alert = probability patient can be successfully treated \* underlying prevalence of event \* cost of outcome avoided if successfully treated (i.e., if the clinicians were doing something more productive with their time)

##### Fixed

```{r}
fx_nmb <- get_nmb_sampler(
  # Cost of ICU admission
  outcome_cost = 14134,
  # Willingness to pay per QALY
  wtp = wtp,
  # QALYs lost due to deterioration event
  qalys_lost = 0.03,
  # Cost of an evaluation = (Clinician time cost * duration of MET) + (Opportunity       cost = chance of successful intervention * outcome cost * underlying p0)
  high_risk_group_treatment_cost = (3.19 * 19) + ((1 - 0.910) * 14134 * p0),
  # Chance of successful intervention
  high_risk_group_treatment_effect = 1 - 0.910
)
```

##### Stochastic:

```{r}
# Need to make a wrapper as the same estimates should be used for multiple cells
sampler <- function() {
  cost_outcome <- rnorm(1, 14134, 686)
  eff_outcome <- 1 - rnorm(1, 0.910, 0.036)
  
  fx_nmb_sampler <- get_nmb_sampler(
    outcome_cost = cost_outcome,
    wtp = wtp,
    qalys_lost = function() rnorm(1, 0.03, 0.04),
    high_risk_group_treatment_cost =
      function() rgamma(1, shape = 110.314, scale = 0.172) * 3.19 +
      (p0 * eff_outcome * cost_outcome),
    high_risk_group_treatment_effect = eff_outcome
  )
  
  fx_nmb_sampler()
}
```

## Derive cutpoints

### ROC-curve and NMB-based cutpoints

```{r}
nmb_simulation <- do_nmb_sim(
  sample_size = sample_size,
  n_sims = 500,
  n_valid = 10000,
  sim_auc = auc,
  event_rate = p0,
  fx_nmb_training = fx_nmb,
  fx_nmb_evaluation = sampler
)
summary(nmb_simulation)
autoplot(nmb_simulation, what = "inb", inb_ref_col = "none") + theme_sim()
autoplot(nmb_simulation, what = "cutpoints") + theme_sim()

cutpoint_youden <- median(nmb_simulation$df_thresholds$youden)
cutpoint_nmb <- median(nmb_simulation$df_thresholds$value_optimising)
```

### PPV-based threshold

```{r, message = FALSE}
roc_curve <- roc(response = sample_pop$actual, predictor = sample_pop$predicted)
ppv <- coords(
  roc_curve, 
  x = "all", 
  input = "threshold", 
  ret = c("threshold", "ppv")
  )

# Maximum tolerable number of false positives per true positive (example)
nne = 4

# NNE = 1/PPV
ppv$nne <- 1/ppv$ppv
cutpoint_nne <- min(ppv$threshold[ppv$nne == nne], na.rm = T)
```

## Apply cutpoints to randomly drawn data

We can now simulate an external validation study under a variety of conditions based on a range of AUC and prevalence values. This mimics a scenario in which we take the original model, developed under the conditions of AUC = 0.85 and p0 of 0.05, and apply the predictions to other datasets, also created using the get_sample() function.

```{r}
combs <- expand.grid(
  auc = c(0.65, 0.75, 0.85, 0.95),
  p0 = c(0.01, 0.05, 0.10, 0.15)
  )

sims <- map2(.x = combs$auc, .y = combs$p0, 
             \(x, y) cbind(
               get_sample(auc = x, 
                          n_samples = 1000, 
                          prevalence = y),
               auc = x,
               p0 = y)
)

source("./utils.R")

# The 'sims' object can then be turned into a set of predictions and classifications (see utils.R)

# In utils, there is a custom classifier() function that compares predicted vs outcome and allocates a class by 2x2 table

df_sims <- obtain_class(sims, samples = 100)

head(df_sims, 10)
```

## Obtain NMB of each strategy

We now have a set of randomly drawn datasets, predictions, and classifications (treat or do not treat). This allows us to specify where each strategy lands on the confusion matrix and therefore what the NMB of each prediction is. We have a stochastic function, fx_nmb_sampler(), so we should use it to generate Monte Carlo samples.

```{r, message = FALSE}
results <- obtain_payoffs(df_sims) |>
  pivot_longer(nmb_classrank5:nmb_classrank25, 
               names_to = "Patients_evaluated", 
               values_to = "NMB") |>
  mutate(Patients_evaluated = as.numeric(gsub(
    "nmb_classrank",
    "",
    Patients_evaluated)))
  

# Plot results
p <- results |> ggplot(aes(x = Patients_evaluated))

p +
  geom_line(aes(y = NMB, colour = "Ranking")) +
  geom_line(aes(y = nmb_val_opt, colour = "Value-optimising threshold")) +
  geom_line(aes(y = nmb_nne, colour = "NNE threshold")) +
  geom_line(aes(y = nmb_youden, colour = "Youden threshold")) +
  scale_colour_manual(name = "Implementation method",
                      values = c(
                        "Ranking" = "#000000",
                        "Value-optimising threshold" = "#999999",
                        "NNE threshold" = "#0072B2",
                        "Youden threshold" = "#E69F00"
                      )) +
  facet_wrap(vars(auc, p0), scales = "free_y")

ggsave(file = "results.png", height = 8, width = 10)
```

## Scenario analysis: Resource-limited applications

Suppose we have identified an appropriate threshold from a cost-effectiveness or number-needed-to-evaluate perspective, but lack the staff to carry out an intervention on all patients identified as high risk. For example, patients who enter a waiting list may be waiting for a limited number of spots. This scenario examines whether there is a benefit to ranking waitlisted patients, rather than selecting randomly from all high-risk individuals.

1.  Draw large samples of (e.g.) 1000 individuals and filter by high risk patients
2.  Select a random sample of e.g. 20 patients who can be actioned based on available resources
3.  Compare this sample to a ranked list of the top 20 patients and obtain the NMB of each example, comparing the payoff across different auc/prevalence values

#### Ideas:

1.  What if we try for a non-hospital example - elective surgery wait lists?
2.  Repeat the analysis for a miscalibrated model?
