---
title: "Ranking method simulation study"
author: "Robin Blythe"
format: html
editor: visual
bibliography: references.bib
---

## Background

This project is an exploratory study to determine whether ranking patients by predicted risk can yield lower costs and better outcomes than using thresholds. Thresholds, while convenient to implement, may be less cost-effective than ranking given that a ranking system should identify true positives first. Prediction models are capable of generating rich outcome data, but dichotomising into high and low risk does not fully utilise predictions.

A recent consideration with regards to implementing thresholds for risk stratification has been to limit the number of alerts based on what clinicians will tolerate, using the positive predictive value (PPV) to determine number needed to evaluate (NNE).[@romero-brufau2015] For example, a model with a PPV of 0.1 corresponds to a rate of 1 positive per 10 alerts, or a NNE of 10. This approach can be considered in addition to other, ROC or value-based methods. [@Parsons2023]

This study contrasts the two approaches - if we randomly sample from a study population, what is the economic impact of applying a ranking approach compared to a threshold approach?

## Study outline

1.  Determine a use case.
    i.  In this study, we will examine the implementation of a program to identify patients at high risk of deterioration leading to additional costs and worse outcomes.
2.  Simulate a hypothetical clinical prediction model with a given AUC and underlying prevalence.
3.  Simulate an underlying patient population using the minimum required sample size.
4.  Obtain costs and outcomes for each state: true positive, true negative, false positive, false negative.
5.  Identify a selection of thresholds based on the study population used to generate the model.
    i.  Value-optimising threshold
    ii. Youden index (or other ROC-based method)
    iii. PPV/NNE-based approach
6.  Randomly generate new sample populations based on a range of plausible AUC and prevalence values to mimic external validation.
7.  Apply the threshold obtained from step 5 and the ranking approach, obtaining costs and outcomes for each sample.
    i.  To apply the ranking approach, need to select a number of patients to evaluate. If the patient's rank \<= number to evaluate, then they are evaluated (true/false positive). If the patient's rank \> number to evaluate, then they are not evaluated (true/false negative).
8.  Compare NMB across variety of AUCs and prevalence for both ranking and threshold-based methods.

## Methods

Let's assume that, left unassessed, a deteriorating medical inpatient will need to be unexpectedly transferred to the ICU, which incurs significant additional costs. A deterioration detection system can escalate care to an outreach team, in this case comprising of an ICU registrar, the patient's attending, and the patient's nurse.

False positives can lead to wasted clinician time, and false negatives can lead to costly ICU admissions. Considering that many patients will require ICU admission regardless of whether they can be successfully intervened with, we should estimate a conservative treatment effect for assessment, rather than assuming an early intervention will necessarily be preventative.

### Set up experiment

```{r, echo = FALSE}
options(scipen = 999, digits = 3)
library(predictNMB)
library(pROC)
library(tidyverse)

# Hypothetical model specs and event rate
auc = 0.85
params = 30
p0 = 0.01
wtp = ceiling(28033*(1.03)^(2024 - 2018))

# Obtain minimum sample size/events
pmsamp <- pmsampsize::pmsampsize(
  type = "b",
  prevalence = p0, 
  cstatistic = auc,
  parameters = params)

sample_size <- pmsamp$sample_size
min_events <- ceiling(pmsamp$events)
```

For a model with `r params` parameters, an AUC of `r auc` and a prevalence of `r p0`, we need a sample size of `r format(sample_size, big.mark = ",")` with `r min_events` events. We can use these requirements to generate a hypothetical study population.

### Generate simulated dataset

```{r}
set.seed(888)

# Use model specs and minimum sample size to generate a sample population
sample_pop <- get_sample(
  auc = auc, 
  n_samples = sample_size, 
  prevalence = p0, 
  min_events = min_events)

sample_pop$actual <- factor(sample_pop$actual)

# Obtain predicted probabilities
fit <- glm(actual ~ x, 
           data = sample_pop, 
           family = binomial()
           )
sample_pop$predicted <- predict(fit, type = "response")
```

### Assign costs and outcomes to 2 x 2 table

#### Parameters

-   Additional costs due to deterioration \~𝒩(14134, 686) [@Curtis2021]
-   Effect of alert on ICU admission rate (hazard ratio) \~𝒩(0.910, 0.036) [@Escobar2020]
-   QALYs lost from deterioration episode \~𝒩(0.03, 0.04) [@Holmes2024]
-   Duration of clinical assessment (using MET call time) \~𝚪(110.314, 0.172) [@Bellomo2003]
    -   Note that this was converted to a Gamma distribution based on a mean duration of 19 minutes and a standard error of 18/√99 = 1.809 minutes using ShinyPrior [@white2023]
-   Cost of clinical time per minute based on ICU outreach registrar, the resident medical officer, and the patient's nurse [@Bohingamu2024]
    -   ICU outreach registrar hourly: 69.91/60 \* (1.03)\^(2024 - 2016) = 1.48
    -   Resident medical officer hourly: 50.79/60 \* (1.03)\^(2024 - 2016) = 1.07
    -   Nurse hourly: 30.47 \* (1.03)\^(2024 - 2016) = 0.64
    -   Altogether: \$3.19/minute (fixed)

##### Fixed

```{r}
fx_nmb <- get_nmb_sampler(
  outcome_cost = 14134,
  wtp = wtp,
  qalys_lost = 0.03,
  high_risk_group_treatment_cost = 3.19 * 19,
  high_risk_group_treatment_effect = 0.910
)
```

##### Stochastic:

```{r}
fx_nmb_sampler <- get_nmb_sampler(
 outcome_cost = function() rnorm(1, 14134, 686),
 wtp = wtp,
 qalys_lost = function() rnorm(1, 0.03, 0.04),
 high_risk_group_treatment_cost = function() rgamma(1, shape = 110.314, scale = 0.172) * 3.19,
 high_risk_group_treatment_effect = function() rnorm(1, 0.910, 0.036)
)
```

## Derive cutpoints

### ROC-curve and NMB-based cutpoints

```{r}
nmb_simulation <- do_nmb_sim(
  sample_size = sample_size,
  n_sims = 500,
  n_valid = 10000,
  sim_auc = auc,
  event_rate = p0,
  fx_nmb_training = fx_nmb,
  fx_nmb_evaluation = fx_nmb_sampler
)
summary(nmb_simulation)
autoplot(nmb_simulation, what = "inb", inb_ref_col = "none") + theme_sim()

cutpoint_youden <- median(nmb_simulation$df_thresholds$youden)
cutpoint_nmb <- median(nmb_simulation$df_thresholds$value_optimising)
```

### PPV-based threshold

```{r, echo = FALSE}
roc_curve <- roc(response = sample_pop$actual, predictor = sample_pop$predicted)
ppv <- coords(
  roc_curve, 
  x = "all", 
  input = "threshold", 
  ret = c("threshold", "ppv")
  )

# Maximum tolerable number of false positives per true positive
nne = 4

# NNE = 1/PPV
ppv$nne <- 1/ppv$ppv
cutpoint_nne <- min(ppv$threshold[ppv$nne == nne], na.rm = T)
```

## Apply cutpoints to randomly drawn data

```{r}
combs <- expand.grid(
  auc = c(0.65, 0.75, 0.85, 0.95),
  p0 = c(0.005, 0.010, 0.015, 0.020)
  )

sims <- map2(.x = combs$auc, .y = combs$p0, 
             \(x, y) cbind(
               get_sample(auc = x, 
                          n_samples = 1000, 
                          prevalence = y),
               auc = x,
               p0 = y)
)

df_sims <- do.call(rbind, sims) |>
  mutate(predicted = predict(fit, newdata = pick(x), type = "response"))
```

#### Ideas:

1.  Could plot X = number of patients seen, Y = NMB, with a line for each approach. NMB would probably be highest for ranking early on, coming equal with other methods over time. This could then be calculated as a table with cost savings for different limits on X (based on clinicians' willingness to respond to alerts).
    1.  Then, could demonstrate how these plots change based on AUC and p0
2.  What if we try for a non-hospital example - elective surgery wait lists?
3.  Maybe we quantify how much time clinicians can dedicate to this (e.g. 15 alerts max) - see Romero-Brufau approach
4.  Repeat the analysis for a miscalibrated model?
