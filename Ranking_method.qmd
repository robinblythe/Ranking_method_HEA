---
title: "Ranking method simulation study"
author: "Robin Blythe"
format: docx
editor: visual
bibliography: references.bib
---

## Background

This project is an exploratory study to determine whether ranking patients by predicted risk can yield lower costs and better outcomes than using thresholds from a hospital perspective. In clinical prediction model implementation, a threshold refers to a predicted value above which the patient is deemed high risk, and below which they are deemed low risk. This can be convenient to implement, as patients can be dichotomised into few risk categories, often with clear recommendations for clinical actions tied to different risk groups.

However, in some clinical settings including acute or emergency care, some patients must be evaluated more urgently than others, and there may be too many high risk patients to evaluate in a timely manner.

This issue can be particularly prevalent in acutely deteriorating inpatients. Many early warning scores for clinical deterioration apply risk strata to both predictors and predictions, leading to tools like the Modified Early Warning Score (MEWS) which scores patients from 0 to 3 in increasing order of risk of adverse events. [@Fullerton2012] The National Early Warning Score 2 (NEWS2) used by the National Health Service uses a wider variety of thresholds, scoring patients from 0 to 20 but considers all scores above 7 to be high risk requiring immediate review, effectively reducing the number of strata as many are considered to be of relatively equal importance. [@Scott2022] While 20 risk groups can lead to more nuanced interpretation than just 2 or 4, this often remains insufficient for clinical practice when individualised risk predictions are available, supporting more nuanced decision making. Nurses are often required to modify or 'individualize' the patient's score to lead to efficient resource allocation, due to strict response protocols for each threshold. [@Langkjaer2021]

We hypothesise that ranking, which retains the continuous scale of predictions and prioritises individuals based on their predicted risk, can be more cost-effective than thresholds. Prediction models are capable of generating important outcome data that are discarded when grouping patients into broad risk categories, as all high-risk patients are considered equal.

This issue has previously been raised in prediction model research in other contexts. Royston et al (2005) demonstrate the consequences of dichotomising continuous predictor variables, or the variables used by the model to generate predicted risks. Dichotomising can be attractive due to its simplicity, but sacrifices considerable statistical power and can lead to both biased estimates and overly narrow estimates of variance. [@Royston2006] Wynants et al (2019) highlight that the use of risk thresholds can lead to severely impaired decision-making for several reasons, including equal weighting for false positives and false negatives, and an inability to consider the preferences of both patients and clinicians. [@Wynants2019] However, prior research has not yet considered the impact of alternatives to threshold selection when a large number of patients must be evaluated simultaneously, for example when prioritising patients for clinical review, and the potential consequences of misclassification in this context.

This simulation study compares a ranking-based approach to a threshold-based approach to determine the potential benefits of prioritising patients based on continuous risk scores.

![Ranking patients by predicted risk can lead to a consistent order in which they are seen, determined by their predicted risk. Threshold-based methods can lead to a random order of evaluation that ignores the nuance of risk predictions.](Ranking%20v%20threshold.jpg){fig-alt="A table that demonstrates how ranking can lead to a consistent order of evaluation based on predicted probabilities when compared to threshold-based approaches." width="787"}

## Study outline

### Setup

1.  Determine a use case.
    i.  In this study, we will examine the implementation of a program to identify patients at high risk of deterioration leading to potentially avoidable additional costs and poor outcomes. We can use the implementation of Escobar et al (2020) to obtain prevalence for ICU admission, a common endpoint for deterioration prediction. [@Escobar2020]
    ii. Clinical deterioration is characterised by several challenges including which outcome to use and, depending on the selected outcome, a low prevalence of the event. [@Blythe2023] However, this has not deterred hospitals from applying clinical deterioration models, despite the lack of success in improving patient outcomes. [@Blythe2022]
2.  Simulate an underlying patient population using the minimum required sample size.
3.  Simulate a hypothetical clinical prediction model with a given AUC and underlying prevalence.
4.  Obtain costs and outcomes for each state: true positive, true negative, false positive, false negative.
5.  Identify a selection of thresholds based on the study population used to generate the model.
    i.  Value-optimising threshold
    ii. Youden index (or other ROC-based method)
    iii. PPV/NNE-based approach
6.  Randomly generate new sample populations based on a range of plausible AUC and prevalence values to mimic new patients being treated with the original model.
7.  Apply the thresholds from step 5, filtering the data to include only high risk patients. Sort patients in order of predicted risk (descending).
8.  Compare the net monetary benefit of treating a random sample of N patients to treating a ranked list of the top N patients.

## Methods

Let's assume that, left unassessed, a deteriorating medical inpatient will need to be rapidly transferred to the ICU, which incurs significant additional costs. A deterioration detection system can escalate that decision to an outreach team, in this case comprising of an ICU registrar, the patient's attending clinician, and the patient's nurse.

False positives lead to wasted clinician time, and false negatives can lead to costly ICU admissions. Considering that many patients will require ICU admission regardless of whether they can be successfully intervened upon, we should estimate a conservative treatment effect for assessment, rather than assuming an early intervention will necessarily be preventative.

### Set up experiment

```{r, message = FALSE}
options(scipen = 999, digits = 3)
library(predictNMB)
library(pROC)
library(tidyverse)

# Hypothetical model specs and event rate
auc = 0.85
params = 30
p0 = 0.05
wtp = 50000

# Obtain minimum sample size/events
pmsamp <- pmsampsize::pmsampsize(
  type = "b",
  prevalence = p0, 
  cstatistic = auc,
  parameters = params)

sample_size <- pmsamp$sample_size
min_events <- ceiling(pmsamp$events)
remove(pmsamp)
```

For a model with `r params` parameters, an AUC of `r auc` and a prevalence of `r p0`, we need a sample size of `r format(sample_size, big.mark = ",")` with `r min_events` events. We can use these requirements to generate a hypothetical study population.

### Generate simulated dataset

```{r}
set.seed(888)
# Use model specs and minimum sample size to generate a sample population
sample_pop <- get_sample(
  auc = auc, 
  n_samples = sample_size, 
  prevalence = p0, 
  min_events = min_events)

sample_pop$actual <- factor(sample_pop$actual)

# Obtain predicted probabilities
fit <- glm(actual ~ x, 
           data = sample_pop, 
           family = binomial()
           )
sample_pop$predicted <- predict(fit, type = "response")
```

### Assign costs and outcomes to 2 x 2 table

#### Parameters

-   Additional costs due to deterioration
    -   The additional cost of deterioration is \~ð’©(14134, 686) [@Curtis2021] (\$AUD)
    -   This cost is in addition to a clinical ICU assessment (see below)
-   Cost of clinical time per minute based on ICU outreach registrar, the resident medical officer, and the patient's nurse [@Bohingamu2024]
    -   ICU outreach registrar hourly: 69.91/60 \* (1.03)\^(2024 - 2016) = 1.48
    -   Resident medical officer hourly: 50.79/60 \* (1.03)\^(2024 - 2016) = 1.07
    -   Nurse hourly: 30.47 \* (1.03)\^(2024 - 2016) = 0.64
    -   Altogether: \$3.19/minute (fixed)
-   Duration of clinical assessment (using MET call time) \~ðšª(110.314, 0.172) [@Bellomo2003]
    -   Note that this was converted to a Gamma distribution based on a mean duration of 19 minutes and a standard error of 18/âˆš99 = 1.809 minutes using ShinyPrior [@white2023]
-   Effect of alert on ICU admission rate (hazard ratio) \~ð’©(0.910, 0.036) [@Escobar2020]
    -   Note: must be taken as (1 - HR) for sampler
-   Average QALYs lost from deterioration episode \~ð’©(0.03, 0.04) [@Holmes2024]
-   Opportunity cost of a positive alert = probability another patient could have been successfully treated \* underlying prevalence of event \* cost of outcome avoided if successfully treated (i.e., if the clinicians were doing something more productive with their time)

##### Fixed estimates for threshold selection

```{r}
fx_nmb <- get_nmb_sampler(
  # Cost of ICU admission
  outcome_cost = 14134,
  # Willingness to pay per QALY
  wtp = wtp,
  # QALYs lost due to deterioration event
  qalys_lost = 0.03,
  # Cost of an evaluation = (Clinician time cost * duration of MET) + (Opportunity       cost = chance of successful intervention * outcome cost * underlying p0)
  high_risk_group_treatment_cost = (3.19 * 19) + ((1 - 0.910) * 14134 * p0),
  # Chance of successful intervention
  high_risk_group_treatment_effect = 1 - 0.910
)
```

## Derive cutpoints

### ROC-curve and NMB-based cutpoints

```{r}
nmb_simulation <- do_nmb_sim(
  sample_size = sample_size,
  n_sims = 500,
  n_valid = 10000,
  sim_auc = auc,
  event_rate = p0,
  fx_nmb_training = fx_nmb,
  fx_nmb_evaluation = fx_nmb
)
summary(nmb_simulation)
autoplot(nmb_simulation, what = "inb", inb_ref_col = "none") + theme_sim()
autoplot(nmb_simulation, what = "cutpoints") + theme_sim()

cutpoint_youden <- median(nmb_simulation$df_thresholds$youden)
cutpoint_nmb <- median(nmb_simulation$df_thresholds$value_optimising)
remove(nmb_simulation)
```

The thresholds derived from our NMB and Youden index approaches are `r round(cutpoint_nmb, 3)` and `r round(cutpoint_youden, 3)`, respectively.

### PPV-based threshold

A recent consideration with regards to implementing thresholds for risk stratification has been to limit the number of alerts based on what clinicians will tolerate, using the positive predictive value (PPV) to determine number needed to evaluate (NNE).[@romero-brufau2015] For example, a model with a PPV of 0.1 corresponds to a rate of 1 positive per 10 alerts, or a NNE of 10. This approach can be considered in addition to other, ROC or value-based methods. [@Parsons2023]

```{r, message = FALSE}
roc_curve <- roc(response = sample_pop$actual, predictor = sample_pop$predicted)
ppv <- coords(
  roc_curve, 
  x = "all", 
  input = "threshold", 
  ret = c("threshold", "ppv")
  )

# Maximum tolerable number of false positives per true positive (example)
nne = 4

# NNE = 1/PPV
ppv$nne <- 1/ppv$ppv
cutpoint_nne <- min(ppv$threshold[ppv$nne == nne], na.rm = T)
remove(roc_curve, ppv, sample_pop)
```

The threshold derived from the number needed to evaluate method is `r round(cutpoint_nmb, 3)`.

## Obtain predictions for variety of external validation scenarios

By repeating the sample population simulation step for a selection of prevalence and AUC values, we can mimic external validation across a variety of external settings, using the original model equation derived from the initial parameterisation.

```{r}
combs <- expand.grid(
  auc = c(0.65, 0.75, 0.85, 0.95),
  p0 = c(0.01, 0.05, 0.10)
  )

sims <- map2(.x = combs$auc, .y = combs$p0, 
             \(x, y) cbind(
               get_sample(auc = x, 
                          n_samples = 1000, 
                          prevalence = y),
               auc = x,
               p0 = y)
)
combs
remove(combs)
```

### Apply cutpoints to classify predictions

We can now simulate an external validation study under a variety of conditions based on a range of AUC and prevalence values. This mimics a scenario in which we take the original model, developed under the conditions of AUC = `r auc` and p0 of `r p0`, and apply the predictions to other datasets, also created using the get_sample() function.

```{r}
source("./utils.R")

# The 'sims' object can then be turned into a set of predictions and classifications (see utils.R)

# In utils, there is a custom classifier() function that compares predicted vs outcome and allocates a class by 2x2 table

df_sims <- obtain_class(sims)
remove(sims)
```

### Obtain NMB of each strategy

We now have a set of randomly drawn datasets, predictions, and classifications (treat or do not treat). However, as the thresholds were drawn using all the data, they may not be suitable when extrapolating to other scenarios, for example a future patient population. In practice, we may often see that a threshold can identify many patients at risk, but we can only see a small number of these patients based on resource constraints (e.g., clinical time), or we don't see enough patients when resources are available (slow periods). Both are inefficient.

We can use the risk predictions to prioritise patients (ranking), or we can just randomly select positive cases. Semi-random selection of positive cases is possibly more common than ranked approaches, especially when there is insufficient communication between clinical staff, who may not be able to differentiate between high risk patients without laying eyes on each. Many patients may be deteriorating, but few can be seen within the recommended timeframe while balancing other tasks.

We can simulate this process by taking random draws. In a large hospital, you may have e.g. 40 patients with a positive prediction for a given threshold. We compare the two approaches by selecting 40 random patients above the threshold and 40 patients sorted by predicted risks.

Method:

1.  For each AUC and prevalence, take 40 random draws for positive cases from each threshold method (NMB, NNE, Youden)
2.  Compare these patients to the top 40 patients by predicted risk, which does not vary.
3.  Compare the positive predictive value by assigning outcomes (TP, FP, TN, FN) to each patient
4.  Repeat steps 1:3 B times for uncertainty estimates

```{r, message = FALSE}
# Set up simulation to run 1000 times
n_samples = 40
max_iter = 1000
sim_results <- list()
for (i in 1:max_iter){
  df_youden <- obtain_sample(df_sims, "class_youden", n_samples)
  df_nne <- obtain_sample(df_sims, "class_nne", n_samples)
  df_val_opt <- obtain_sample(df_sims, "class_val_opt", n_samples)
  
  df_rank <- df_sims |>
    group_by(auc, p0) |>
    arrange(desc(predicted)) |>
    slice_head(n = n_samples) |>
    mutate(Method = "ranking") |>
    select(Method, auc, p0, predicted, actual) 
  
  iteration <- do.call(rbind, list(df_youden, df_nne, df_val_opt, df_rank)) |>
    group_by(Method, auc, p0) |>
    rowwise() |>
    mutate(Outcome = ifelse(actual == 1, "TP", "FP"),
           Cost = ifelse(
             Outcome == "FP",
             # Opportunity cost of a false positive (see methods section)
             rgamma(1, shape = 110.314, scale = 0.172) * 3.19 + 
               (p0 * (1 - rnorm(1, 0.910, 0.036)) * rnorm(1, 14134, 686)),
             # No cost for true positives (for now)
             0),
           iter = i)
  
  sim_results[[i]] <- iteration
  remove(df_youden, df_nne, df_val_opt, df_rank, iteration)
}

results <- do.call(rbind, sim_results)
head(results, 15)
```

We now have simulations for each starting scenario (`r n_samples` samples for each AUC, prevalence, and threshold selection method over `r max_iter` bootstrap iterations).

```{r, message = FALSE}
# Summarise results for each level of AUC and prevalence
results$TP_rate <- ifelse(results$Outcome == "TP", 1, 0)
results <- results |>
  group_by(Method, auc, p0, iter) |>
  summarise(PPV = mean(TP_rate),
            Cost = mean(Cost)) |>
  group_by(Method, auc, p0) |>
  summarise(PPV_median = median(PPV),
            PPV_low = quantile(PPV, 0.025),
            PPV_high = quantile(PPV, 0.975),
            Cost_median = median(Cost),
            Cost_low = quantile(Cost, 0.025),
            Cost_high = quantile(Cost, 0.975)) |>
  rename(PPV = PPV_median,
         Cost = Cost_median) |>
  mutate(Method = case_when(
    Method == "nne" ~ "NNE-based threshold",
    Method == "val_opt" ~ "Value-optimising threshold",
    Method == "youden" ~ "Youden index threshold",
    Method == "ranking" ~ "Ranking by predicted risk"
  ))
head(results, 10)
```

```{r, message = FALSE}
# Create plots
p0_values <- c(`0.01` = "Prevalence = 0.01",
               `0.05` = "Prevalence = 0.05",
               `0.1` = "Prevalence = 0.10")

p_PPV <- results |> 
  ggplot(aes(x = auc, y = PPV, colour = Method, fill = Method))

p_PPV +
  geom_line(linewidth = 2) +
  geom_ribbon(aes(ymin = PPV_low, ymax = PPV_high), 
              alpha = 0.2, linetype = "dotted"
              ) +
  facet_wrap(vars(p0), labeller = as_labeller(p0_values)) +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        legend.position = "bottom") +
  scale_x_continuous(limits = c(0.64, 0.96),
                     breaks = seq(0.65, 0.95, 0.1), 
                     name = "Model AUC")

ggsave(filename = "PPV_results.png", height = 6, width = 10)

p_costs <- results |>
  ggplot(aes(x = auc, y = Cost, colour = Method, fill = Method))

p_costs +
  geom_line(linewidth = 2) +
  geom_ribbon(aes(ymin = Cost_low, ymax = Cost_high), 
              alpha = 0.2, linetype = "dotted"
              ) +
  facet_wrap(vars(p0), labeller = as_labeller(p0_values)) +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        legend.position = "bottom") +
  scale_x_continuous(limits = c(0.64, 0.96), 
                     breaks = seq(0.65, 0.95, 0.1), 
                     name = "Model AUC") +
  scale_y_continuous(breaks = seq(25, 200, 25), 
                     name = "Mean misclassification cost per patient")

ggsave(filename = "Cost_results.png", height = 6, width = 10)
```

## Classify Singaporean ED data using thresholds

To use a worked example, we can take real ED data from Singapore and a predictive model for mortality, SERP. We load the data, fit a logistic regression to assign probabilities to the SERP scores (\~61 unique levels in our data), then run the threshold estimation process on this.

```{r}
# Load data
df_ed <- readRDS(file = "C:/Users/blythe/NUS Dropbox/EDData/ED2A/ED2A_Blythe Robin Daniel/dat_scored.RDS")

# Double check AUC
auc(response = df_ed$outcome, predictor = df_ed$pred_risk)

# Data contains 61 unique values. Not quite 100, but not bad. Need to convert to probabilities for predictNMB to work:
fit_serp <- glm(
  outcome ~ pred_risk,
  data = df_ed,
  family = binomial()
)

df_ed$pred_prob <- predict(fit_serp, type = "response")

# Obtain costs and outcomes of classification
# Should probably get a proper Singaporean dataset/example together. These are too retrofitted to the US example
fx_nmb_serp <- get_nmb_sampler(
  # Cost of unnecessary ICU admission
  outcome_cost = 12015 * (1.03)^(2025 - 2021),
  # Willingness to pay per QALY - Singapore
  wtp = 50000,
  # QALYs lost due to deterioration event
  qalys_lost = 0.03,
  # Cost of an evaluation = (Clinician time cost * duration of MET * AUD to SGD conversion rate) + (Opportunity cost = chance of successful intervention * outcome cost * underlying p0)
  high_risk_group_treatment_cost = (3.19 * 19 * 0.85) + ((1 - 0.910) * 12015 * (1.03)^(2025 - 2021) * p0),
  # Chance of successful intervention
  high_risk_group_treatment_effect = 1 - 0.910
)

# Apply thresholds
serp_thresholds <- get_thresholds(
  predicted = df_ed$pred_prob,
  actual = df_ed$outcome,
  nmb = fx_nmb_serp()
)

# Get probability thresholds
serp_nmb <- serp_thresholds[["value_optimising"]]
serp_youden <- serp_thresholds[["youden"]]
# Convert back to SERP values
prob_table <- data.frame(pred_risk = seq(1, 100, 1))
prob_table$pred_prob <- predict(fit_serp, type = "response", newdata = prob_table)
cutpoint_serp_nmb <- prob_table$pred_risk[prob_table$pred_prob == serp_nmb]
cutpoint_serp_youden <- prob_table$pred_risk[prob_table$pred_prob == serp_youden]
remove(serp_thresholds, fit_serp)

# Add PPV threshold
roc_curve <- roc(response = df_ed$outcome, predictor = df_ed$pred_risk)
ppv <- coords(
  roc_curve, 
  x = "all", 
  input = "threshold", 
  ret = c("threshold", "ppv")
  )

# NNE = 1/PPV
ppv$nne <- 1/ppv$ppv
cutpoint_serp_nne <- floor(min(ppv$threshold[ppv$nne == nne], na.rm = T))
remove(roc_curve, ppv)
```

## Worked example results

Now that we have data from a real example, we can estimate the results as per the simulation above. Assuming the SGH ED sees 300 patients per day, there may be n patients there at peak time. If n = 50, we can take a random sample of 50 patients and compare the PPV and opportunity costs of applying SERP with thresholds and with ranking.

```{r}
df_ed$class_val_opt <- ifelse(df_ed$pred_risk >= cutpoint_serp_nmb, 1, 0)
df_ed$class_nne <- ifelse(df_ed$pred_risk >= cutpoint_serp_nne, 1, 0)
df_ed$class_youden <- ifelse(df_ed$pred_risk >= cutpoint_serp_youden, 1, 0)

n_samples <- 50
max_iter <- 1000

serp_results <- list()
for (i in 1:max_iter) {
  serp_results[[i]] <- sample(df_ed, size = n_samples)
}


```
