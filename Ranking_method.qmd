---
title: "Ranking method simulation study"
author: "Robin Blythe"
format: docx
editor: visual
bibliography: references.bib
---

## Background

This project is an exploratory study to determine whether ranking patients by predicted risk can yield lower costs and better outcomes than using thresholds from a hospital perspective. In clinical prediction model implementation, a threshold refers to a predicted value above which the patient is deemed high risk, and below which they are deemed low risk. This can be convenient to implement, as patients can be dichotomised into few risk categories, often with clear recommendations for clinical actions tied to different risk groups.

However, in some clinical settings including acute or emergency care, some patients must be evaluated more urgently than others, and there may be too many high risk patients to evaluate in a timely manner.

This issue can be particularly prevalent in acutely deteriorating inpatients. Many early warning scores for clinical deterioration apply risk strata to both predictors and predictions, leading to tools like the Modified Early Warning Score (MEWS) which scores patients from 0 to 3 in increasing order of risk of adverse events. [@Fullerton2012] The National Early Warning Score 2 (NEWS2) used by the National Health Service uses a wider variety of thresholds, scoring patients from 0 to 20 but considers all scores above 7 to be high risk requiring immediate review, effectively reducing the number of strata as many are considered to be of relatively equal importance. [@Scott2022] While 20 risk groups can lead to more nuanced interpretation than just 2 or 4, this often remains insufficient for clinical practice when individualised risk predictions are available, supporting more nuanced decision making. Nurses are often required to modify or 'individualize' the patient's score to lead to efficient resource allocation, due to strict response protocols for each threshold. [@Langkjaer2021]

We hypothesise that ranking, which retains the continuous scale of predictions and prioritises individuals based on their predicted risk, can be more cost-effective than thresholds. Prediction models are capable of generating important outcome data that are discarded when grouping patients into broad risk categories, as all high-risk patients are considered equal.

This issue has previously been raised in prediction model research in other contexts. Royston et al (2005) demonstrate the consequences of dichotomising continuous predictor variables, or the variables used by the model to generate predicted risks. Dichotomising can be attractive due to its simplicity, but sacrifices considerable statistical power and can lead to both biased estimates and overly narrow estimates of variance. [@Royston2006] Wynants et al (2019) highlight that the use of risk thresholds can lead to severely impaired decision-making for several reasons, including equal weighting for false positives and false negatives, and an inability to consider the preferences of both patients and clinicians. [@Wynants2019] However, prior research has not yet considered the impact of alternatives to threshold selection when a large number of patients must be evaluated simultaneously, for example when prioritising patients for clinical review, and the potential consequences of misclassification in this context.

This simulation study compares a ranking-based approach to a threshold-based approach to determine the potential benefits of prioritising patients based on continuous risk scores.

![Ranking patients by predicted risk can lead to a consistent order in which they are seen, determined by their predicted risk. Threshold-based methods can lead to a random order of evaluation that ignores the nuance of risk predictions.](Ranking%20v%20threshold.jpg){fig-alt="A table that demonstrates how ranking can lead to a consistent order of evaluation based on predicted probabilities when compared to threshold-based approaches." width="787"}

## Study outline

### Setup

1.  Determine a use case.
    i.  In this study, we will examine the implementation of a program to identify patients at high risk of deterioration leading to potentially avoidable additional costs and poor outcomes. We can use the implementation of Escobar et al (2020) to obtain prevalence for ICU admission, a common endpoint for deterioration prediction. [@Escobar2020]
    ii. Clinical deterioration is characterised by several challenges including which outcome to use and, depending on the selected outcome, a low prevalence of the event. [@Blythe2023] However, this has not deterred hospitals from applying clinical deterioration models, despite the lack of success in improving patient outcomes. [@Blythe2022]
2.  Simulate an underlying patient population using the minimum required sample size.
3.  Simulate a hypothetical clinical prediction model with a given AUC and underlying prevalence.
4.  Obtain costs and outcomes for each state: true positive, true negative, false positive, false negative.
5.  Identify a selection of thresholds based on the study population used to generate the model.
    i.  Value-optimising threshold
    ii. Youden index (or other ROC-based method)
    iii. PPV/NNE-based approach
6.  Randomly generate new sample populations based on a range of plausible AUC and prevalence values to mimic new patients being treated with the original model.
7.  Apply the thresholds from step 5, filtering the data to include only high risk patients. Sort patients in order of predicted risk (descending).
8.  Compare the net monetary benefit of treating a random sample of N patients to treating a ranked list of the top N patients.

## Methods

Let's assume that, left unassessed, a deteriorating medical inpatient will need to be rapidly transferred to the ICU, which incurs significant additional costs. A deterioration detection system can escalate that decision to an outreach team, in this case comprising of an ICU registrar, the patient's attending clinician, and the patient's nurse.

False positives lead to wasted clinician time, and false negatives can lead to costly ICU admissions. Considering that many patients will require ICU admission regardless of whether they can be successfully intervened upon, we should estimate a conservative treatment effect for assessment, rather than assuming an early intervention will necessarily be preventative.

### Set up experiment

```{r, message = FALSE}
options(scipen = 999, digits = 3)
library(predictNMB)
library(pROC)
library(tidyverse)

# Hypothetical model specs and event rate
auc = 0.85
params = 30
p0 = 0.05

# Obtain minimum sample size/events
pmsamp <- pmsampsize::pmsampsize(
  type = "b",
  prevalence = p0, 
  cstatistic = auc,
  parameters = params)

sample_size <- pmsamp$sample_size
min_events <- ceiling(pmsamp$events)
remove(pmsamp)
```

For a model with `r params` parameters, an AUC of `r auc` and a prevalence of `r p0`, we need a sample size of `r format(sample_size, big.mark = ",")` with `r min_events` events. We can use these requirements to generate a hypothetical study population.

### Generate simulated dataset

```{r}
set.seed(888)
# Use model specs and minimum sample size to generate a sample population
sample_pop <- get_sample(
  auc = auc, 
  n_samples = sample_size, 
  prevalence = p0, 
  min_events = min_events)

sample_pop$actual <- factor(sample_pop$actual)

# Obtain predicted probabilities
fit <- glm(actual ~ x, 
           data = sample_pop, 
           family = binomial()
           )
sample_pop$predicted <- predict(fit, type = "response")
```

### Assign costs and outcomes to 2 x 2 table

#### Parameters

-   Additional costs due to deterioration
    -   The additional cost of deterioration is \~𝒩(14134, 686) [@Curtis2021] (\$AUD)
    -   This cost is in addition to a clinical ICU assessment (see below)
-   Cost of clinical time per minute based on ICU outreach registrar, the resident medical officer, and the patient's nurse [@Bohingamu2024]
    -   ICU outreach registrar hourly: 69.91/60 \* (1.03)\^(2024 - 2016) = 1.48
    -   Resident medical officer hourly: 50.79/60 \* (1.03)\^(2024 - 2016) = 1.07
    -   Nurse hourly: 30.47 \* (1.03)\^(2024 - 2016) = 0.64
    -   Altogether: \$3.19/minute (fixed)
-   Duration of clinical assessment (using MET call time) \~𝚪(110.314, 0.172) [@Bellomo2003]
    -   Note that this was converted to a Gamma distribution based on a mean duration of 19 minutes and a standard error of 18/√99 = 1.809 minutes using ShinyPrior [@white2023]
-   Effect of alert on ICU admission rate (hazard ratio) \~𝒩(0.910, 0.036) [@Escobar2020]
    -   Note: must be taken as (1 - HR) for sampler
-   Average QALYs lost from deterioration episode \~𝒩(0.03, 0.04) [@Holmes2024]
-   Opportunity cost of a positive alert = probability another patient could have been successfully treated \* underlying prevalence of event \* cost of outcome avoided if successfully treated (i.e., if the clinicians were doing something more productive with their time)

##### Fixed estimates for threshold selection

```{r}
wtp = 45000

fx_nmb <- get_nmb_sampler(
  # Cost of ICU admission
  outcome_cost = (14134*0.85)*(1.03)^4,
  # Willingness to pay per QALY
  wtp = wtp,
  # QALYs lost due to deterioration event
  qalys_lost = 0.03,
  # Cost of an evaluation = (Clinician time cost * duration of MET) + (Opportunity       cost = chance of successful intervention * outcome cost * underlying p0)
  high_risk_group_treatment_cost = (3.19 * 0.85 * 1.03 * 19) + ((1 - 0.910) * (14134*0.85)*(1.03)^4 * p0),
  # Chance of successful intervention
  high_risk_group_treatment_effect = 1 - 0.910
)
```

## Derive cutpoints

### ROC-curve and NMB-based cutpoints

```{r}
nmb_simulation <- do_nmb_sim(
  sample_size = sample_size,
  n_sims = 500,
  n_valid = 10000,
  sim_auc = auc,
  event_rate = p0,
  fx_nmb_training = fx_nmb,
  fx_nmb_evaluation = fx_nmb
)
summary(nmb_simulation)
autoplot(nmb_simulation, what = "inb", inb_ref_col = "none") + theme_sim()
autoplot(nmb_simulation, what = "cutpoints") + theme_sim()

cutpoint_youden <- median(nmb_simulation$df_thresholds$youden)
cutpoint_nmb <- median(nmb_simulation$df_thresholds$value_optimising)
remove(nmb_simulation)
```

The thresholds derived from our NMB and Youden index approaches are `r round(cutpoint_nmb, 3)` and `r round(cutpoint_youden, 3)`, respectively.

### PPV-based threshold

A recent consideration with regards to implementing thresholds for risk stratification has been to limit the number of alerts based on what clinicians will tolerate, using the positive predictive value (PPV) to determine number needed to evaluate (NNE).[@romero-brufau2015] For example, a model with a PPV of 0.1 corresponds to a rate of 1 positive per 10 alerts, or a NNE of 10. This approach can be considered in addition to other, ROC or value-based methods. [@Parsons2023]

```{r, message = FALSE}
roc_curve <- roc(response = sample_pop$actual, predictor = sample_pop$predicted)
ppv <- coords(
  roc_curve, 
  x = "all", 
  input = "threshold", 
  ret = c("threshold", "ppv")
  )

# Maximum tolerable number of false positives per true positive (example)
nne = 4

# NNE = 1/PPV
ppv$nne <- 1/ppv$ppv
cutpoint_nne <- min(ppv$threshold[ppv$nne == nne], na.rm = T)
remove(roc_curve, ppv, sample_pop)
```

The threshold derived from the number needed to evaluate method is `r round(cutpoint_nmb, 3)`.

## Obtain predictions for variety of external validation scenarios

By repeating the sample population simulation step for a selection of prevalence and AUC values, we can mimic external validation across a variety of external settings, using the original model equation derived from the initial parameterisation.

```{r}
combs <- expand.grid(
  auc = c(0.65, 0.75, 0.85, 0.95),
  p0 = c(0.01, 0.05, 0.10)
  )

sims <- map2(.x = combs$auc, .y = combs$p0, 
             \(x, y) cbind(
               get_sample(auc = x, 
                          n_samples = 1000, 
                          prevalence = y),
               auc = x,
               p0 = y)
)
remove(combs)
```

### Apply cutpoints to classify predictions and obtain NMB of each strategy

We can now simulate an external validation study under a variety of conditions based on a range of AUC and prevalence values. This mimics a scenario in which we take the original model, developed under the conditions of AUC = `r auc` and p0 of `r p0`, and apply the predictions to other datasets, also created using the get_sample() function.

We now have a set of randomly drawn datasets, predictions, and classifications (treat or do not treat). However, as the thresholds were drawn using all the data, they may not be suitable when extrapolating to other scenarios, for example a future patient population. In practice, we may often see that a threshold can identify many patients at risk, but we can only see a small number of these patients based on resource constraints (e.g., clinical time), or we don't see enough patients when resources are available (slow periods). Both are inefficient.

We can use the risk predictions to prioritise patients (ranking), or we can just randomly select positive cases. Semi-random selection of positive cases is possibly more common than ranked approaches, especially when there is insufficient communication between clinical staff, who may not be able to differentiate between high risk patients without laying eyes on each. Many patients may be deteriorating, but few can be seen within the recommended timeframe while balancing other tasks.

We can simulate this process by taking random draws. In a large hospital, you may have e.g. 40 patients with a positive prediction for a given threshold. We compare the two approaches by selecting 40 random patients above the threshold and 40 patients sorted by predicted risks.

Method:

1.  For each AUC and prevalence, take 40 random draws for positive predictions from each threshold method (NMB, NNE, Youden)
2.  Compare these patients to the top 40 patients by predicted risk, which does not vary.
3.  Compare the positive predictive value by assigning outcomes (TP, FP, TN, FN) to each patient
4.  Repeat steps 1:3 B times for uncertainty estimates

```{r, message = FALSE}
# Set up simulation
source("./utils.R")

n_samples = 40
max_iter = 10000
sim_results <- list()

for (i in 1:max_iter){
  df_sims <- obtain_class(sims)

  df_youden <- obtain_sample(df_sims, "class_youden", n_samples)
  df_nne <- obtain_sample(df_sims, "class_nne", n_samples)
  df_val_opt <- obtain_sample(df_sims, "class_val_opt", n_samples)
  
  df_rank <- df_sims |>
    group_by(auc, p0) |>
    arrange(desc(predicted)) |>
    slice_head(n = n_samples) |>
    mutate(Method = "ranking") |>
    select(Method, auc, p0, predicted, actual) 
  
  iteration <- do.call(rbind, list(df_youden, df_nne, df_val_opt, df_rank)) |>
    group_by(Method, auc, p0) |>
    rowwise() |>
    mutate(Outcome = ifelse(actual == 1, "TP", "FP"),
           Cost = ifelse(
             Outcome == "FP",
             # Opportunity cost of a false positive (see methods section)
             rgamma(1, shape = 110.314, scale = 0.172) * 3.19 + 
               (p0 * (1 - rnorm(1, 0.910, 0.036)) * rnorm(1, 14134, 686)),
             # No cost for true positives (for now)
             0),
           iter = i)
  
  sim_results[[i]] <- iteration
  remove(df_youden, df_nne, df_val_opt, df_rank, iteration)
}

results <- do.call(rbind, sim_results)
remove(sims)
```

We now have simulations for each starting scenario (`r n_samples` samples for each AUC, prevalence, and threshold selection method over `r max_iter` bootstrap iterations).

```{r, message = FALSE}
# Summarise results for each level of AUC and prevalence
results$TP_rate <- ifelse(results$Outcome == "TP", 1, 0)
results <- results |>
  group_by(Method, auc, p0, iter) |>
  summarise(PPV = mean(TP_rate),
            Cost = mean(Cost)) |>
  group_by(Method, auc, p0) |>
  summarise(PPV_median = median(PPV),
            PPV_low = quantile(PPV, 0.025),
            PPV_high = quantile(PPV, 0.975),
            Cost_median = median(Cost),
            Cost_low = quantile(Cost, 0.025),
            Cost_high = quantile(Cost, 0.975)) |>
  rename(PPV = PPV_median,
         Cost = Cost_median) |>
  mutate(Method = case_when(
    Method == "nne" ~ "NNE-based threshold",
    Method == "val_opt" ~ "Value-optimising threshold",
    Method == "youden" ~ "Youden index threshold",
    Method == "ranking" ~ "Ranking by predicted risk"
  ))
saveRDS(results, file = "sim_results.RDS")
```

```{r, message = FALSE}
# Create plots
library(patchwork)
p0_values <- c(`0.01` = "Prevalence = 0.01",
               `0.05` = "Prevalence = 0.05",
               `0.1` = "Prevalence = 0.10")

p_PPV <- results |> 
  ggplot(aes(x = auc, y = PPV, colour = Method, fill = Method))

p_costs <- results |>
  ggplot(aes(x = auc, y = Cost, colour = Method, fill = Method))

p_PPV +
  geom_line(linewidth = 2, show.legend = FALSE) +
  geom_ribbon(aes(ymin = PPV_low, ymax = PPV_high), 
              alpha = 0.2, linetype = "dotted",
              show.legend = FALSE
              ) +
  facet_wrap(vars(p0), labeller = as_labeller(p0_values)) +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        legend.position = "bottom") +
  scale_x_continuous(limits = c(0.64, 0.96),
                     breaks = seq(0.65, 0.95, 0.1), 
                     name = "Model AUC") +
  guides(y = "none") +
p_costs +
  geom_line(linewidth = 2) +
  geom_ribbon(aes(ymin = Cost_low, ymax = Cost_high), 
              alpha = 0.2, linetype = "dotted"
              ) +
  facet_wrap(vars(p0), labeller = as_labeller(p0_values)) +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        legend.position = "bottom") +
  scale_x_continuous(limits = c(0.64, 0.96), 
                     breaks = seq(0.65, 0.95, 0.1), 
                     name = "Model AUC") +
  scale_y_continuous(breaks = seq(25, 200, 25), 
                     name = "Mean misclassification cost per patient") +
  plot_layout(ncol = 1) +
  plot_annotation(tag_levels = 'A')
  

ggsave(filename = "Sim_results.png", height = 10, width = 8)

rm(list = ls(all.names = TRUE))
gc()
```

## Classify Singaporean ED data using thresholds

To use a worked example, we can take real ED data from Singapore and a predictive model for mortality, SERP. We load the data, fit a logistic regression to assign probabilities to the SERP scores (\~61 unique levels in our data), then run the threshold estimation process on this.

### Get thresholds

```{r, message = FALSE}
# Load data
df_ed <- readRDS("C:/Users/Robin/NUS Dropbox/EDData/ED2A/ED2A_Blythe Robin Daniel/dat_scored.RDS") # home laptop
#df_ed <- readRDS("C:/Users/blythe/NUS Dropbox/EDData/ED2A/ED2A_Blythe Robin Daniel/dat_scored.RDS") # work laptop

# Obtain thresholds from https://www.nature.com/articles/s41598-022-22233-w#Tab3
library(pROC)
serp_roc <- roc(response = df_ed$outcome_died_30d, predictor = df_ed$pred_risk)
cutpoint_serp <- coords(
  roc = serp_roc,
  x = "all",
  input = "threshold",
  ret = c("threshold", "specificity", "ppv")
)

# Inspect serp_roc to find thresholds
threshold_spec_70 <- 24
threshold_spec_80 <- 27
threshold_spec_90 <- 31

# Apply thresholds to population
library(dplyr)
df_ed <- df_ed |>
  mutate(
    t1 = ifelse(pred_risk >= threshold_spec_70, 1, 0),
    t2 = ifelse(pred_risk >= threshold_spec_80, 1, 0),
    t3 = ifelse(pred_risk >= threshold_spec_90, 1, 0),
    class_t1 = case_when(
      t1 == 1 & outcome_died_30d == 1 ~ "TP",
      t1 == 1 & outcome_died_30d == 0 ~ "FP",
      t1 == 0 & outcome_died_30d == 1 ~ "FN",
      t1 == 0 & outcome_died_30d == 0 ~ "TN"
    ),
    class_t2 = case_when(
      t2 == 1 & outcome_died_30d == 1 ~ "TP",
      t2 == 1 & outcome_died_30d == 0 ~ "FP",
      t2 == 0 & outcome_died_30d == 1 ~ "FN",
      t2 == 0 & outcome_died_30d == 0 ~ "TN"
    ),
    class_t3 = case_when(
      t3 == 1 & outcome_died_30d == 1 ~ "TP",
      t3 == 1 & outcome_died_30d == 0 ~ "FP",
      t3 == 0 & outcome_died_30d == 1 ~ "FN",
      t3 == 0 & outcome_died_30d == 0 ~ "TN"
    )
  )

remove(serp_roc, cutpoint_serp, threshold_spec_70, threshold_spec_80, threshold_spec_90)
```

While we are at it, we should check model performance first based on discrimination and calibration. This will determine how close our example is to the simulated ideal.

```{r}
library(ggplot2)
preds <- subset(df_ed, select = c("outcome_died_30d", "pred_risk"))
preds$pr_prob <- predict(glm(outcome_died_30d ~ pred_risk, family = binomial(), data = preds), type = "response")
auc(predictor = preds$pr_prob, response = preds$outcome_died_30d)

p_calib_30d <- preds |> ggplot()
p_calib_30d +
  geom_histogram(aes(x = pr_prob, y = ..count../sum(..count..)), alpha = 0.2, colour = "#999999") +
  geom_smooth(aes(x = pr_prob, y = outcome_died_30d)) +
  geom_abline() +
  theme_bw() +
  ylab("Observed frequency") +
  xlab("Predicted probability") +
  geom_text(aes(x = 0.05, y = 0.9), label = "Model AUC (30d mortality) = 0.83") +
  geom_text(aes(x = 0.4, y = 0.65), label = "Model underestimates risks") +
  geom_text(aes(x = 0.6, y = 0.25), label = "Model overestimates risks")

```

### Results

Now that we have data from a real setting, we can estimate the results as per the simulation above. In our example, the ED may see around 135,000 patients a year, or 370/day. Assuming there are busy times throughout the day, there may periods where as many as 150 patients are in the ED at any given time.

The highest acuity patients are typically evaluated urgently. This leaves many patients who may be too acutely unwell for discharge, but lower priority. In a busy ED, this may require some prioritisation of resources to avoid deterioration events for those patients who are waiting for assessment. Prospectively assessing based on predicted risk compared to assessing randomly (e.g., by order of ED presentation) may lead to better model performance, indicating that it is not just the model but how it is implemented that can drive outcomes.

### Approach 1: repeat the simulated analysis

Let's repeat the simulation first to test whether our results stack up using real data.

Note - currently, we are using the model to predict 30 day mortality as a proxy for clinical deterioration. When we instead predict ICU admission (perhaps a better measure), you end up with lower benefits because the model AUC is more like 0.65. This supports our findings from before - so we should replicate the analysis and use it for both.

```{r}
# Custom function simulator() ->
# 1. Obtain random set of patients with risk > t1 and risk < t3
# 2. Obtain randomly selected set for evaluation
# 3. Compare to ranked set for evaluation
source("./utils.R")
set.seed(888)
n_samples = 150

sim_n_1 <- simulator(
  data = df_ed,
  n_samples = n_samples,
  n_eval = ceiling(0.05 * n_samples),
  max_iter = 10000
  )

sim_n_2 <- simulator(
  data = df_ed,
  n_samples = n_samples,
  n_eval = ceiling(0.10 * n_samples),
  max_iter = 10000 
)

sim_n_3 <- simulator(
  data = df_ed,
  n_samples = n_samples,
  n_eval = ceiling(0.15 * n_samples),
  max_iter = 10000 
)

sim_n_1$group <- "5% of patients evaluated"
sim_n_2$group <- "10% of patients evaluated"
sim_n_3$group <- "15% of patients evaluated"

sims_summary <- bind_rows(sim_n_1, sim_n_2, sim_n_3) |>
  mutate(group = factor(group, levels = c("5% of patients evaluated",
                                          "10% of patients evaluated",
                                          "15% of patients evaluated"))) |>
  group_by(group, Method, iter) |>
  summarise(Cost = mean(Cost),
            PPV = mean(TP)) |>
  group_by(group, Method) |>
  summarise(Mean_PPV = mean(PPV),
            PPV_low = quantile(PPV, 0.025),
            PPV_high = quantile(PPV, 0.975),
            Mean_cost = mean(Cost),
            Cost_low = quantile(Cost, 0.025),
            Cost_high = quantile(Cost, 0.975)) |>
  ungroup()

d_costs <- sims_summary |>
  group_by(group) |>
  summarise(Savings_annual = (Mean_cost[2] - Mean_cost[1]) * 135000,
            Savings_low = (Cost_low[2] - Cost_low[1]) * 135000,
            Savings_high = (Cost_high[2] - Cost_high[1]) * 135000) 


```

## Repeating analysis for ICU admission prediction

As noted above, the model is well calibrated and a good discriminator for 30 day mortality. However, it may be used in a context where ICU admission is the more useful predicted outcome. We should repeat the analysis to demonstrate this.

```{r}
options(scipen = 100, digits = 3)
library(ggplot2)
library(patchwork)
preds_icu <- subset(df_ed, select = c("outcome_icu", "pred_risk"))
preds_icu$pr_prob <- predict(glm(outcome_icu ~ pred_risk, family = binomial(), data = preds_icu), type = "response")
auc(predictor = preds_icu$pr_prob, response = preds_icu$outcome_icu)

p_calib_icu <- preds_icu |> ggplot()

p_calib_icu +
  geom_histogram(aes(x = pr_prob, y = ..count../sum(..count..)), alpha = 0.2, colour = "#999999") +
  geom_smooth(aes(x = pr_prob, y = outcome_icu), colour = "#D55E00") +
  geom_abline() +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.25)) +
  theme_bw() +
  ylab("Observed frequency") +
  xlab("Predicted probability") +
  geom_text(aes(x = 0.5, y = 0.9), label = "Model AUC (ICU admission) = 0.65") +
  geom_text(aes(x = 0.4, y = 0.65), label = "Model underestimates risks") +
  geom_text(aes(x = 0.6, y = 0.25), label = "Model overestimates risks") +
p_calib_30d +
  geom_histogram(aes(x = pr_prob, y = ..count../sum(..count..)), alpha = 0.2, colour = "#999999") +
  geom_smooth(aes(x = pr_prob, y = outcome_died_30d)) +
  geom_abline() +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.25)) +
  theme_bw() +
  ylab("Observed frequency") +
  xlab("Predicted probability") +
  geom_text(aes(x = 0.5, y = 0.9), label = "Model AUC (30d mortality) = 0.83") +
  geom_text(aes(x = 0.4, y = 0.65), label = "Model underestimates risks") +
  geom_text(aes(x = 0.6, y = 0.25), label = "Model overestimates risks") +
plot_layout(ncol = 1) +
  plot_annotation(tag_levels = 'A')

ggsave(file = "SERP_model_results.jpg", height = 10, width = 8)


########################
library(pROC)
serp_roc <- roc(response = df_ed$outcome_icu, predictor = df_ed$pred_risk)
cutpoint_serp <- coords(
  roc = serp_roc,
  x = "all",
  input = "threshold",
  ret = c("threshold", "specificity", "ppv")
)

# Inspect serp_roc to find thresholds
threshold_spec_70_icu <- 24
threshold_spec_80_icu <- 27
threshold_spec_90_icu <- 32

# Apply thresholds to population
library(dplyr)
df_ed <- df_ed |>
  mutate(
    t1 = ifelse(pred_risk >= threshold_spec_70_icu, 1, 0),
    t2 = ifelse(pred_risk >= threshold_spec_80_icu, 1, 0),
    t3 = ifelse(pred_risk >= threshold_spec_90_icu, 1, 0),
    class_t1 = case_when(
      t1 == 1 & outcome_icu == 1 ~ "TP",
      t1 == 1 & outcome_icu == 0 ~ "FP",
      t1 == 0 & outcome_icu == 1 ~ "FN",
      t1 == 0 & outcome_icu == 0 ~ "TN"
    ),
    class_t2 = case_when(
      t2 == 1 & outcome_icu == 1 ~ "TP",
      t2 == 1 & outcome_icu == 0 ~ "FP",
      t2 == 0 & outcome_icu == 1 ~ "FN",
      t2 == 0 & outcome_icu == 0 ~ "TN"
    ),
    class_t3 = case_when(
      t3 == 1 & outcome_icu == 1 ~ "TP",
      t3 == 1 & outcome_icu == 0 ~ "FP",
      t3 == 0 & outcome_icu == 1 ~ "FN",
      t3 == 0 & outcome_icu == 0 ~ "TN"
    )
  )

remove(serp_roc, cutpoint_serp, threshold_spec_70_icu, threshold_spec_80_icu, threshold_spec_90_icu)


source("./utils.R")
set.seed(888)
n_samples = 150

sim_n_1_icu <- simulator(
  data = df_ed,
  n_samples = n_samples,
  n_eval = ceiling(0.05 * n_samples),
  max_iter = 10000
  )

sim_n_2_icu <- simulator(
  data = df_ed,
  n_samples = n_samples,
  n_eval = ceiling(0.10 * n_samples),
  max_iter = 10000 
)

sim_n_3_icu <- simulator(
  data = df_ed,
  n_samples = n_samples,
  n_eval = ceiling(0.15 * n_samples),
  max_iter = 10000 
)

sim_n_1_icu$group <- "5% of patients evaluated"
sim_n_2_icu$group <- "10% of patients evaluated"
sim_n_3_icu$group <- "15% of patients evaluated"

sims_icu_summary <- bind_rows(sim_n_1_icu, sim_n_2_icu, sim_n_3_icu) |>
  mutate(group = factor(group, levels = c("5% of patients evaluated",
                                          "10% of patients evaluated",
                                          "15% of patients evaluated"))) |>
  group_by(group, Method, iter) |>
  summarise(Cost = mean(Cost),
            PPV = mean(TP)) |>
  group_by(group, Method) |>
  summarise(Mean_PPV = mean(PPV),
            PPV_low = quantile(PPV, 0.025),
            PPV_high = quantile(PPV, 0.975),
            Mean_cost = mean(Cost),
            Cost_low = quantile(Cost, 0.025),
            Cost_high = quantile(Cost, 0.975)) |>
  ungroup()

d_costs_icu <- sims_icu_summary |>
  group_by(group) |>
  summarise(Savings_annual = (Mean_cost[2] - Mean_cost[1]) * 135000,
            Savings_low = (Cost_low[2] - Cost_low[1]) * 135000,
            Savings_high = (Cost_high[2] - Cost_high[1]) * 135000)

# Next step - got to plot the sims_icu results. Do this next time
```

### Approach 2: Try something different (rank vs threshold)

Experiment: what if we use ranking instead of thresholds?

Similar to above: the model AUC plays a large part in this. Replicate the below analysis for ICU admission too

```{r} n_samples = 150 wtp = 50000 max_iter = 10000 ranking_out <- list()  set.seed(888) for (i in 1:max_iter) {    FP = rgamma(1, shape = 110.314, scale = 0.172) * (3.19 * 0.85)   FN = rnorm(1, 14134, 686) * 0.85 + (wtp * rnorm(1, 0.03, 0.04))    # Sample from data   df = df_ed[sample(nrow(df_ed), n_samples),] |>     arrange(desc(pred_risk))    ranking_out[[i]] <- tibble(     iter = rep(i, 7),     Method = c(       "Rank 5", "Rank 10", "Rank 15", "Rank 20", "Unranked t1", "Unranked t2", "Unranked t3"     ),     Mean_cost = c(       (5 - sum(df[1:5,]$outcome_died_30d)) * FP +         sum(df[6:n_samples,]$outcome_died_30d) * FN,       (10 - sum(df[1:10,]$outcome_died_30d)) * FP +         sum(df[11:n_samples,]$outcome_died_30d) * FN,       (15 - sum(df[1:15,]$outcome_died_30d)) * FP +         sum(df[16:n_samples,]$outcome_died_30d) * FN,       (20 - sum(df[1:20,]$outcome_died_30d)) * FP +         sum(df[21:n_samples,]$outcome_died_30d) * FN,       sum(df$class_t1 == "FP") * FP +         sum(df$class_t1 == "FN") * FN,       sum(df$class_t2 == "FP") * FP +         sum(df$class_t2 == "FN") * FN,       sum(df$class_t3 == "FP") * FP +         sum(df$class_t3 == "FN") * FN     ),     Mean_PPV = c(       mean(df[1:5,]$outcome_died_30d),       mean(df[1:10,]$outcome_died_30d),       mean(df[1:15,]$outcome_died_30d),       mean(df[1:20,]$outcome_died_30d),       sum(df$class_t1 == "TP")/sum(df$t1),       sum(df$class_t2 == "TP")/sum(df$t2),       sum(df$class_t3 == "TP")/sum(df$t3)       )   ) }  rank_results <- do.call(rbind, ranking_out) rank_summary <- rank_results |>   na.omit() |>   group_by(Method) |>   summarise(Cost = median(Mean_cost),             Cost_low = quantile(Mean_cost, 0.025),             Cost_high = quantile(Mean_cost, 0.975),             PPV = median(Mean_PPV),             PPV_low = quantile(Mean_PPV, 0.025),             PPV_high = quantile(Mean_PPV, 0.975))}
```

## 
