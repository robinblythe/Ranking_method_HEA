---
title: "Ranking method simulation study"
author: "Robin Blythe"
format: html
editor: visual
bibliography: references.bib
---

## Background

This project is an exploratory study to determine whether ranking patients by predicted risk can yield lower costs and better outcomes than using thresholds. Thresholds, while convenient to implement, may be less cost-effective than ranking given that a ranking system should identify true positives first. Prediction models are capable of generating rich outcome data, but dichotomising into high and low risk do not utilise the information being produced.

A recent consideration with regards to implementing thresholds for risk stratification has been to limit the number of alerts based on what clinicians will tolerate. This approach can be considered in addition to other, ROC or value-based methods. [@Parsons2023]

This study contrasts the two approaches - if we randomly sample from a study population, what is the economic impact of applying a ranking approach compared to a threshold approach?

## Study outline

1.  Determine a use case.
    i.  In this study, we will examine the implementation of a program to identify patients at high risk of deterioration leading to additional costs and worse outcomes.
2.  Simulate a hypothetical clinical prediction model with a given AUC and underlying prevalence.
3.  Simulate an underlying patient population using the minimum required sample size.
4.  Obtain costs and outcomes for each state: true positive, true negative, false positive, false negative.
5.  Identify a selection of thresholds based on the study population used to generate the model.
6.  Randomly generate new sample populations based on a range of plausible AUC and prevalence values to mimic external validation.
7.  Apply the threshold obtained from step 5 and the ranking approach, obtaining costs and outcomes for each sample.
8.  Compare NMB across variety of AUCs and prevalence for both ranking and threshold-based methods.

## Methods

Let's assume that, left unassessed, a deteriorating medical inpatient will need to be unexpectedly transferred to the ICU, which incurs significant additional costs. A deterioration detection system can escalate care to an outreach team; false positives can lead to wasted clinician time, and false negatives can lead to costly ICU admissions.

Considering that many patients will require ICU admission regardless of whether they can be successfully intervened with, we should estimate a conservative treatment effect for assessment, rather than assuming an early intervention will necessarily be preventative.

### Set up experiment

```{r}
options(scipen = 999, digits = 3)
library(predictNMB)
library(pROC)

# Hypothetical model specs and event rate
auc = 0.85
params = 30
p0 = 0.01
wtp = ceiling(28033*(1.03)^(2024 - 2018))

# Obtain minimum sample size/events
pmsamp <- pmsampsize::pmsampsize(
  type = "b",
  prevalence = p0, 
  cstatistic = auc,
  parameters = params)

sample_size <- pmsamp$sample_size
min_events <- ceiling(pmsamp$events)
```

For a model with `r params` parameters, an AUC of `r auc` and a prevalence of `r p0`, we need a sample size of `r format(sample_size, big.mark = ",")` with `r min_events` events. We can use these requirements to generate a hypothetical study population.

### Generate simulated dataset

```{r}
set.seed(888)

# Use model specs and minimum sample size to generate a sample population
sample_pop <- get_sample(
  auc = auc, 
  n_samples = sample_size, 
  prevalence = p0, 
  min_events = min_events)

# Obtain predicted probabilities
fit <- glm(actual ~ x, 
           data = sample_pop, 
           family = binomial()
           )
sample_pop$predicted <- predict(fit, type = "response")
```

### Assign costs and outcomes to 2 x 2 table

#### Parameters

-   Additional costs due to deterioration \~ð’©(14134, 686) [@Curtis2021]
-   Effect of alert on ICU admission \~ð’©(0.910, 0.036) [@Escobar2020]
-   QALYs lost from deterioration episode \~ð’©(0.03, 0.04) [@Holmes2024]
-   Duration of clinical assessment (using MET call time) \~ðšª(110.314, 0.172) [@Bellomo2003]
    -   Note that this was converted to a Gamma distribution based on a mean duration of 19 minutes and a standard error of 18/âˆš99 = 1.809 minutes using ShinyPrior [@white2023]
-   Cost of clinical time per minute based on ICU outreach registrar, the resident medical officer, and the patient's nurse [@Bohingamu2024]
    -   ICU outreach registrar hourly: 69.91/60 \* (1.03)\^(2024 - 2016) = 1.48
    -   Resident medical officer hourly: 50.79/60 \* (1.03)\^(2024 - 2016) = 1.07
    -   Nurse hourly: 30.47 \* (1.03)\^(2024 - 2016) = 0.64
    -   Altogether: \$3.19/minute (fixed)

#### Fixed:

```{r}
fx_nmb <- get_nmb_sampler(
  outcome_cost = 14134,
  wtp = wtp,
  qalys_lost = 0.03,
  high_risk_group_treatment_cost = 3.19 * 19,
  high_risk_group_treatment_effect = 0.910
)
```

#### Stochastic:

```{r}
fx_nmb_sampler <- get_nmb_sampler(
 outcome_cost = function() rnorm(1, 14134, 686),
 wtp = wtp,
 qalys_lost = function() rnorm(1, 0.03, 0.04),
 high_risk_group_treatment_cost = function() rgamma(1, shape = 110.314, scale = 0.172) * 3.19,
 high_risk_group_treatment_effect = function() rnorm(1, 0.910, 0.036)
)
```

#### Derive cutpoints

```{r}
nmb_simulation <- do_nmb_sim(
  sample_size = sample_size,
  n_sims = 500,
  n_valid = 10000,
  sim_auc = auc,
  event_rate = p0,
  fx_nmb_training = fx_nmb,
  fx_nmb_evaluation = fx_nmb_sampler
)
summary(nmb_simulation)
autoplot(nmb_simulation, what = "inb", inb_ref_col = "none") + theme_sim()
autoplot(nmb_simulation, what = "cutpoints") + theme_sim()
```

#### Ideas:

1.  Could plot X = number of patients seen, Y = NMB, with a line for each approach. NMB would probably be highest for ranking early on, coming equal with other methods over time. This could then be calculated as a table with cost savings for different limits on X (based on clinicians' willingness to respond to alerts).
    1.  Then, could demonstrate how these plots change based on AUC and p0
2.  What if we try for a non-hospital example - elective surgery wait lists?
3.  Maybe we quantify how much time clinicians can dedicate to this (e.g. 15 alerts max) - see Romero-Brufau approach
4.  Repeat the analysis for a miscalibrated model?
